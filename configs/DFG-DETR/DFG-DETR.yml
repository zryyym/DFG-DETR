__include__: [
  '/media/student/stu/zh/DEIM-main-3/configs/DFG-DETR/base-dfine_hgnetv2_n_coco.yml',
  '/media/student/stu/zh/DEIM-main-3/configs/base/deim.yml'
]
## Model FLOPs:7.4643 GFLOPS   MACs:3.6916 GMACs   Params:3953645
output_dir: ./deim_outputs/exp7-base-DirDown-AFR-DSSA

DEIM:
  encoder: HybridEncoder_fusion

project_name: ablation3
exp_name: exp7-base-DirDown-AFR-DSSA

HGNetv2:
  agg: ca
  block: test
  gate: 4
  down: enhance
  ca: 1

HybridEncoder_fusion:
  att: dssa
  enhancer: True
#  down: enhanced
  in_channels: [512, 1024]
  feat_strides: [16, 32]

  # intra
  hidden_dim: 128
  use_encoder_idx: [1]
  num_encoder_layers: 1
  nhead: 8
  dim_feedforward: 512
  dropout: 0.
  enc_act: 'gelu'

  # cross
  expansion: 0.34
  depth_mult: 0.5
  act: 'silu'


optimizer:
  type: AdamW
  params:
    -
      params: '^(?=.*backbone)(?!.*norm|bn).*$'
      lr: 0.0004
    -
      params: '^(?=.*backbone)(?=.*norm|bn).*$'
      lr: 0.0004
      weight_decay: 0.
    -
      params: '^(?=.*(?:encoder|decoder))(?=.*(?:norm|bn|bias)).*$'
      weight_decay: 0.

  lr: 0.0008
  betas: [0.9, 0.999]
  weight_decay: 0.0001

# Increase to search for the optimal ema
epoches: 300 # 148 + 12

## Our LR-Scheduler
flat_epoch: 144    # 4 + epoch // 2, e.g., 40 = 4 + 72 / 2
no_aug_epoch: 20
lr_gamma: 1.0
warmup_iter: 2000

## Our DataAug
num_classes: 9
train_dataloader:
  total_batch_size: 16
  dataset:
    transforms:
      policy:
        epoch: [4, 160, 280]   # list

  collate_fn:
    mixup_epochs: [4, 160]
    stop_epoch: 280
    base_size_repeat: ~

val_dataloader:
  total_batch_size: 16